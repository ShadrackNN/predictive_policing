{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a0feef9-c9d0-4aa6-aae4-ff0b64dd4d00",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m crime_data = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mODC_CRIME_OFFENSES_P_-3254178225590307312.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Optionally load GeoJSON for geospatial previews or merges (contains point geometries for crimes).\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m geo_crime = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mODC_CRIME_OFFENSES_P_-5582264493798559810.geojson\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Preview the CSV data to understand structure (why: Ensures data loaded correctly, shows columns like dates, neighborhoods).\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCSV Data Preview:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\file.py:316\u001b[39m, in \u001b[36m_read_file\u001b[39m\u001b[34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m             filename = response.read()\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyogrio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfiona\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_file_like(filename):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\geopandas\\io\\file.py:576\u001b[39m, in \u001b[36m_read_file_pyogrio\u001b[39m\u001b[34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m     warnings.warn(\n\u001b[32m    568\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[33m'\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mignore_fields\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keywords are deprecated, and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future release. You can use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    572\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    573\u001b[39m     )\n\u001b[32m    574\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m] = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33minclude_fields\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pyogrio\\geopandas.py:355\u001b[39m, in \u001b[36mread_dataframe\u001b[39m\u001b[34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    351\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    353\u001b[39m meta, index, geometry, field_data = result\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m columns = \u001b[43mmeta\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfields\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m data = {columns[i]: field_data[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(columns))}\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fid_as_index:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# pandas: Needed for data manipulation, loading CSV/GeoJSON, grouping, and creating DataFrames.\n",
    "import pandas as pd\n",
    "\n",
    "# geopandas: Needed for handling geospatial data from GeoJSON, merging with crime data for mapping.\n",
    "import geopandas as gpd\n",
    "\n",
    "# numpy: Needed for numerical operations, like rounding predictions.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib.pyplot: Needed for creating basic plots like bar charts.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn: Needed for advanced visualizations like countplots and heatmaps.\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn.linear_model.LinearRegression: Needed for the linear regression model.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# sklearn.ensemble.RandomForestRegressor: Needed for the random forest regression model.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# sklearn.neighbors.KNeighborsRegressor: Needed for the KNN regression model.\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# sklearn.svm.SVR: Needed for the support vector regression model.\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# sklearn.neural_network.MLPRegressor: Needed for the multi-layer perceptron (neural network) regression model.\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# xgboost.XGBRegressor: Needed for the XGBoost regression model.\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load the CSV data (main dataset with crime details).\n",
    "crime_data = pd.read_csv('ODC_CRIME_OFFENSES_P_-3254178225590307312.csv')\n",
    "\n",
    "# Optionally load GeoJSON for geospatial previews or merges (contains point geometries for crimes).\n",
    "geo_crime = gpd.read_file('ODC_CRIME_OFFENSES_P_-5582264493798559810.geojson')\n",
    "\n",
    "# Preview the CSV data to understand structure (why: Ensures data loaded correctly, shows columns like dates, neighborhoods).\n",
    "print(\"CSV Data Preview:\")\n",
    "print(crime_data.head())\n",
    "\n",
    "# Preview the GeoJSON data (why: Confirms geospatial points for potential mapping).\n",
    "print(\"\\nGeoJSON Data Preview:\")\n",
    "print(geo_crime.head())\n",
    "\n",
    "# Parse the 'FIRST_OCCURRENCE_DATE' to datetime (why: Allows extraction of year, day, hour for temporal analysis).\n",
    "crime_data['FIRST_OCCURRENCE_DATE'] = pd.to_datetime(crime_data['FIRST_OCCURRENCE_DATE'], errors='coerce')\n",
    "\n",
    "# Extract year for train/test split (why: Use historical years for training, current for testing to simulate real prediction).\n",
    "crime_data['Year'] = crime_data['FIRST_OCCURRENCE_DATE'].dt.year\n",
    "\n",
    "# Extract DayOfWeek and Hour (why: Key features for grouping and predicting crime patterns by time).\n",
    "crime_data['DayOfWeek'] = crime_data['FIRST_OCCURRENCE_DATE'].dt.day_name()\n",
    "crime_data['Hour'] = crime_data['FIRST_OCCURRENCE_DATE'].dt.hour\n",
    "\n",
    "# Filter to actual crimes and drop NaNs in key columns (why: Focus on relevant data, avoid model errors from missing values).\n",
    "crime_data = crime_data[crime_data['IS_CRIME'] == 1].dropna(subset=['NEIGHBORHOOD_ID', 'Hour', 'DayOfWeek'])\n",
    "\n",
    "# Preview engineered features (why: Verify transformations worked).\n",
    "print(\"\\nEngineered Features Preview:\")\n",
    "print(crime_data[['FIRST_OCCURRENCE_DATE', 'DayOfWeek', 'Hour', 'NEIGHBORHOOD_ID']].head())\n",
    "\n",
    "# Visualization: Crime count by hour (why: Provides insight into temporal patterns, e.g., peaks at night for policing focus).\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Hour', data=crime_data)\n",
    "plt.title('Crime Distribution by Hour in Denver')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Add 'Crimes' column for counting (why: Each row represents one crime; this allows aggregation).\n",
    "crime_data['Crimes'] = 1\n",
    "\n",
    "# Group by year, day, neighborhood, hour and sum crimes (why: Aggregates to predictable units like 'crimes in Five Points on Monday at 8pm').\n",
    "hour_totals = crime_data.groupby(['Year', 'DayOfWeek', 'NEIGHBORHOOD_ID', 'Hour'])['Crimes'].sum().reset_index()\n",
    "\n",
    "# Split into train (pre-2025) and test (2025) (why: Train on past data, test on current to evaluate predictive power).\n",
    "train = hour_totals[hour_totals['Year'] < 2025]\n",
    "test = hour_totals[hour_totals['Year'] == 2025]\n",
    "\n",
    "# Create dummy variables for categorical features (why: Models require numerical inputs; one-hot encoding handles categories like days/neighborhoods).\n",
    "train_dummies = pd.get_dummies(train[['Crimes', 'Hour', 'DayOfWeek', 'NEIGHBORHOOD_ID']])\n",
    "test_dummies = pd.get_dummies(test[['Crimes', 'Hour', 'DayOfWeek', 'NEIGHBORHOOD_ID']])\n",
    "\n",
    "# Align test columns to train (fill missing with 0) (why: Ensures same features in both sets for model compatibility).\n",
    "test_dummies = test_dummies.reindex(columns=train_dummies.columns, fill_value=0)\n",
    "\n",
    "# Define X and y for train/test (why: Standard setup for supervised learning; X=features, y=target).\n",
    "X_train = train_dummies.drop('Crimes', axis=1)\n",
    "y_train = train_dummies['Crimes']\n",
    "X_test = test_dummies.drop('Crimes', axis=1)\n",
    "y_test = test_dummies['Crimes']\n",
    "\n",
    "# Visualization: Heatmap of average crimes by day and hour (why: Visualizes patterns, e.g., weekend evenings hotter, aiding explanation).\n",
    "plt.figure(figsize=(14, 8))\n",
    "pivot = hour_totals.pivot_table(index='DayOfWeek', columns='Hour', values='Crimes', aggfunc='mean')\n",
    "sns.heatmap(pivot, cmap='YlOrRd')\n",
    "plt.title('Average Crimes by Day and Hour in Denver')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Define models dictionary (why: Allows easy iteration to train/compare multiple regressors for best performance).\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),  # Simple linear relationship model\n",
    "    'RandomForestRegressor': RandomForestRegressor(),  # Ensemble of decision trees, good for complex patterns\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),  # Distance-based, predicts based on similar instances\n",
    "    'SVR': SVR(),  # Support Vector Regression, finds optimal boundary for prediction\n",
    "    'XGBRegressor': XGBRegressor(),  # Gradient boosting, sequential tree building for high accuracy\n",
    "    'MLPRegressor': MLPRegressor(hidden_layer_sizes=(100,100,100,100), random_state=444)  # Neural network model\n",
    "}\n",
    "\n",
    "# Train and evaluate each model (why: Compare R2 scores to select best for predictions; higher score = better fit).\n",
    "print(\"Model Performance (R2 Scores):\")\n",
    "print(\"R2 Score Range: -∞ to 1.0 (higher is better)\")\n",
    "print(\"0.0 = model predicts mean, <0.0 = worse than mean prediction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store scores for analysis\n",
    "scores = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    scores[name] = score\n",
    "    print(f'{name}: {score:.4f}')\n",
    "\n",
    "# Identify best performing model based on R2 score\n",
    "best_model_name = max(scores, key=scores.get)\n",
    "best_model = models[best_model_name]\n",
    "best_score = scores[best_model_name]\n",
    "\n",
    "# Sort models by performance for interpretation\n",
    "sorted_models = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} with R2 Score: {best_score:.4f}\")\n",
    "\n",
    "# Dynamic R2 Score Interpretation\n",
    "print(f\"\\nR2 Score Interpretation:\")\n",
    "print(f\"- {sorted_models[0][0]} ({sorted_models[0][1]:.4f}): Best performer, explains {sorted_models[0][1]*100:.1f}% of variance in crime data\")\n",
    "\n",
    "if len(sorted_models) > 1:\n",
    "    print(f\"- {sorted_models[1][0]} ({sorted_models[1][1]:.4f}): Second best, explains {sorted_models[1][1]*100:.1f}% of variance\")\n",
    "\n",
    "# Performance categorization\n",
    "print(f\"\\nPerformance Categories:\")\n",
    "for i, (name, score) in enumerate(sorted_models):\n",
    "    if score >= 0.3:\n",
    "        category = \"Good\"\n",
    "    elif score >= 0.2:\n",
    "        category = \"Moderate\" \n",
    "    elif score >= 0.1:\n",
    "        category = \"Weak\"\n",
    "    else:\n",
    "        category = \"Poor\"\n",
    "    print(f\"- {name}: {category} (R2 = {score:.4f})\")\n",
    "\n",
    "# Visualization: Feature importances from Random Forest (why: Shows which features (e.g., neighborhoods) drive predictions, building interpretability).\n",
    "if 'RandomForestRegressor' in models:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    importances = pd.Series(models['RandomForestRegressor'].feature_importances_, index=X_train.columns)\n",
    "    importances.nlargest(10).plot(kind='barh')\n",
    "    plt.title('Top 10 Feature Importances for Crime Prediction (Random Forest)')\n",
    "    plt.xlabel('Importance Score (higher = more influential)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate predictions using the BEST model (why: To compare predicted vs actual crimes using the most accurate model).\n",
    "test = test.copy()  # Create explicit copy to avoid SettingWithCopyWarning\n",
    "test.loc[:, 'Predicted'] = best_model.predict(X_test)\n",
    "\n",
    "# Round predictions (why: Crime counts are integers; improves readability).\n",
    "test = np.round(test, 2)\n",
    "\n",
    "print(f\"\\nUsing {best_model_name} for final predictions (best R2 score: {best_score:.4f})\")\n",
    "\n",
    "# Export predictions to JSON (why: For interactive visualizations, e.g., in D3.js maps as in SF example).\n",
    "test.to_json('denver_crime_predictions.json', orient='records', double_precision=2)\n",
    "print(\"Predictions exported to 'denver_crime_predictions.json'\")\n",
    "\n",
    "# Additional performance summary\n",
    "print(f\"\\n=== Performance Summary ===\")\n",
    "print(f\"Models tested: {len(scores)}\")\n",
    "print(f\"Score range: {min(scores.values()):.4f} - {max(scores.values()):.4f}\")\n",
    "print(f\"Average R2 score: {np.mean(list(scores.values())):.4f}\")\n",
    "\n",
    "# Visualization: Bar plot of actual vs predicted by neighborhood (why: Compares model accuracy visually per area).\n",
    "# Melt for side-by-side bars.\n",
    "plt.figure(figsize=(14, 8))\n",
    "melted = pd.melt(test, id_vars=['NEIGHBORHOOD_ID'], value_vars=['Crimes', 'Predicted'], var_name='Type', value_name='Count')\n",
    "sns.barplot(x='NEIGHBORHOOD_ID', y='Count', hue='Type', data=melted)\n",
    "plt.title(f'Actual vs Predicted Crimes by Neighborhood (2025 Test Data)\\nUsing {best_model_name} Model (R² = {best_score:.3f})')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Merge with GeoJSON for chloropleth (why: Spatial viz of predictions; requires neighborhood shapes, but uses points here as proxy).\n",
    "print(\"\\nAttempting spatial merge for chloropleth visualization...\")\n",
    "print(\"geo_crime columns:\", list(geo_crime.columns))\n",
    "print(\"test columns:\", list(test.columns))\n",
    "\n",
    "# Extract Hour from datetime columns in geo_crime to match test dataset\n",
    "datetime_cols = ['FIRST_OCCURRENCE_DATE', 'LAST_OCCURRENCE_DATE', 'REPORTED_DATE']\n",
    "available_datetime_cols = [col for col in datetime_cols if col in geo_crime.columns]\n",
    "\n",
    "if available_datetime_cols:\n",
    "    # Use the first available datetime column to extract hour\n",
    "    datetime_col = available_datetime_cols[0]\n",
    "    print(f\"Using '{datetime_col}' to extract hour information\")\n",
    "    \n",
    "    # Convert to datetime and extract hour\n",
    "    geo_crime_copy = geo_crime.copy()\n",
    "    geo_crime_copy['Hour'] = pd.to_datetime(geo_crime_copy[datetime_col]).dt.hour\n",
    "    print(f\"Extracted hours range: {geo_crime_copy['Hour'].min()} to {geo_crime_copy['Hour'].max()}\")\n",
    "    \n",
    "    # Now merge on both NEIGHBORHOOD_ID and Hour\n",
    "    merge_on = ['NEIGHBORHOOD_ID', 'Hour']\n",
    "    print(f\"Merging on columns: {merge_on}\")\n",
    "    \n",
    "    # Perform the merge\n",
    "    merged_geo = geo_crime_copy.merge(test, on=merge_on, how='left', suffixes=('_geo', '_pred'))\n",
    "    \n",
    "    # Check if merge was successful and Predicted column exists\n",
    "    if 'Predicted' in merged_geo.columns:\n",
    "        # Create the chloropleth plot\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Plot by predicted crime density\n",
    "        ax = merged_geo.plot(column='Predicted', \n",
    "                            cmap='OrRd', \n",
    "                            legend=True, \n",
    "                            alpha=0.8, \n",
    "                            edgecolor='black', \n",
    "                            linewidth=0.5,\n",
    "                            figsize=(14, 10),\n",
    "                            legend_kwds={'label': 'Predicted Crime Count', \n",
    "                                       'orientation': 'horizontal',\n",
    "                                       'shrink': 0.8})\n",
    "        \n",
    "        plt.title(f'Predicted Crimes Distribution by Neighborhood and Hour\\nUsing {best_model_name} Model (R² = {best_score:.3f})', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add some context to the plot\n",
    "        plt.figtext(0.5, 0.01, \n",
    "                   f\"Data points: {merged_geo['Predicted'].notna().sum()} | \"\n",
    "                   f\"Prediction range: {merged_geo['Predicted'].min():.1f} to {merged_geo['Predicted'].max():.1f}\",\n",
    "                   ha='center', fontsize=10, style='italic')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed statistics about the predictions\n",
    "        predicted_data = merged_geo['Predicted'].dropna()\n",
    "        print(f\"\\n=== Spatial Prediction Statistics ===\")\n",
    "        print(f\"Total spatial points with predictions: {len(predicted_data)}\")\n",
    "        print(f\"Predicted crimes - Min: {predicted_data.min():.2f}\")\n",
    "        print(f\"Predicted crimes - Max: {predicted_data.max():.2f}\")\n",
    "        print(f\"Predicted crimes - Mean: {predicted_data.mean():.2f}\")\n",
    "        print(f\"Predicted crimes - Std: {predicted_data.std():.2f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: 'Predicted' column not found after merge.\")\n",
    "else:\n",
    "    print(\"Note: Could not create spatial visualization - hour extraction from geo_crime failed.\")\n",
    "\n",
    "print(\"\\n=== Analysis Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43a8a8-09ea-4dbe-ac2f-ca941be854d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
